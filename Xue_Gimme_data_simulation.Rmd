---
title: "STAT293_GIMME"
author: "Yijia Xue"
date: "2025-11-21"
output: pdf_document
editor_options: 
  markdown: 
    wrap: 72
---

#1. Data Simulation \################################
\################################

```{r}
library(MASS)
set.seed(123)
```

# (1). Generate white noise $\zeta$

```{r}
generate_Psi_i <- function(A_i,
                           min_k = 1,
                           max_k = 4,
                           base_var = 1,
                           low_cov = 0.05,
                           high_cov = 0.2) {
  p <- nrow(A_i)
  Psi <- diag(base_var, p, p)
  
  # candidate off-diagonal positions where A_i has zero and use only i < j to enforce symmetry
  candidates <- which(A_i == 0, arr.ind = TRUE)
  candidates <- candidates[candidates[,1] < candidates[,2], , drop = FALSE]
  if (nrow(candidates) == 0) return(Psi)
  
  k <- sample(min_k:max_k, 1)
  k <- min(k, nrow(candidates))
  chosen_idx <- sample(1:nrow(candidates), k, replace = FALSE)
  chosen <- candidates[chosen_idx, , drop = FALSE]
  
  # assign random covariances at those positions
  for (m in 1:nrow(chosen)) {
    i <- chosen[m, 1]
    j <- chosen[m, 2]
    val <- runif(1, low_cov, high_cov)
    Psi[i, j] <- val
    Psi[j, i] <- val
  }
  
  # ensure positive definite 
  ev <- eigen(Psi, symmetric = TRUE)$values
  if (min(ev) <= 0) {
    jitter <- (abs(min(ev)) + 1e-3)
    Psi <- Psi + jitter * diag(p)
  }

  return(Psi)
}

```


# (2). Simulate one subject given $A_i$ and $\Phi_i$

$\eta_{t} = A_{i}\,\eta_{t} + \Phi_{i}\,\eta_{t-1} + \zeta_{t}$

$[\eta_{t}=(I - A_{i})^{-1}\left[\Phi_{i}\,\eta_{t-1} + \zeta_{t}\right]$

```{r}
simulate_subject <- function(T, A, Phi, Psi) {
  p <- nrow(A)
  X <- matrix(NA, nrow = T, ncol = p)
  colnames(X) <- paste0("X", 1:p)

  # initial state eta_1 ~ N(0, I)
  X[1, ] <- runif(p,min=0, max=0.5)

  # (I - A)^(-1)
  M <- solve(diag(p) - A)

  for (t in 2:T) {
    zeta_t <- mvrnorm(1, mu = rep(0, p), Sigma = Psi)  #zeta is the white noise
    X[t, ] <- M %*% (Phi %*% X[t - 1, ] + zeta_t)
  }

  X
}
```

############################################ 

# (3). data simulation

```{r}
set.seed(123)

p<-8 # number of variables
T<-300+100 # time points per subject
N<-100# number of subjects

#diagonal residual covariance 
Psi<- 0.05*diag(p)

A_common <- matrix(0, p, p)

for (j in 1:(p-1)) {
  A_common[j+1, j] <- runif(1,0.4,0.5)
}

diag(A_common) <- 0


Phi_common <- matrix(0, p, p)

# AR(1) on each variable
diag(Phi_common) <- runif(p, min = 0.15, max = 0.25)

# add some shared cross-lag effects (example pattern)

random_k <- function(min_k, max_k) {
  sample(min_k:max_k, size = 1)
}

for (j in 1:(p-1)) {
  Phi_common[j+1, j] <- runif(1, 0.05, 0.12)
}

for (j in 1:(p-2)) {
  Phi_common[j+2, j] <- runif(1, 0.05, 0.10)
}
extra_edges <- 3
for (k in 1:extra_edges) {
  r <- sample(1:p, 1)
  c <- sample(setdiff(1:p, r), 1)
  Phi_common[r, c] <- runif(1, 0.03, 0.08)
}

generate_random_individual_edges <- function(M_common,
                                             min_k = 4,
                                             max_k = 8,
                                             low=0.03,
                                             high=0.3,
                                             allow_diag = FALSE) {

  p <- nrow(M_common)
  M <- M_common

  # ---- 1. Choose how many individual edges this subject gets ----
  k_indiv <- random_k(min_k, max_k)

  # ---- 2. Find eligible positions (cannot overwrite group edges) ----
  possible <- which(M_common == 0, arr.ind = TRUE)

  # optionally remove diagonal
  if (!allow_diag) {
    possible <- possible[possible[,1] != possible[,2], , drop = FALSE]
  }

  # safety: cannot choose more edges than exist
  k_indiv <- min(k_indiv, nrow(possible))

  # ---- 3. Randomly select k edges ----
  idx <- sample(1:nrow(possible), size = k_indiv, replace = FALSE)
  chosen <- possible[idx, , drop = FALSE]

  # ---- 4. Fill with random values ----
  for (j in 1:nrow(chosen)) {
    to   <- chosen[j, 1]
    from <- chosen[j, 2]
    M[to, from] <- runif(1,low,high)
  }

  # ensure no diagonals for A
  if (!allow_diag) diag(M) <- 0

  return(M)
}



A_list   <- vector("list", N)
Phi_list <- vector("list", N)
X_list   <- vector("list", N)
Psi_list <- vector("list", N)

for (s in 1:N) {

  # A_i: no diagonal allowed
  A_i <- generate_random_individual_edges(
    M_common=A_common,
    min_k = 1,     # random number between 1 and 3
    max_k = 3,
    low = 0,
    high = 0.3,
    allow_diag = FALSE
  )

  # enforce: no bidirectional contemporaneous edges
  p <- nrow(A_i)
  for (i in 1:p) {
    for (j in 1:p) {
      if (i < j && A_i[i, j] != 0 && A_i[j, i] != 0) {
        # keep i<-j, drop j<-i (you could choose the opposite if you prefer)
        A_i[j, i] <- 0
      }
    }
  }

  
  # Phi_i: diagonal allowed (AR terms)
  Phi_i <- generate_random_individual_edges(
    M_common = Phi_common,
    min_k = 4,
    max_k = 8,     
    low = 0.03,
    high = 0.3,
    allow_diag = TRUE
  )
  
  Psi_i <- generate_Psi_i(
    A_i,
    min_k = 1,
    max_k = 4,
    base_var = 0.5,
    low_cov = 0.05,
    high_cov = 0.2
  )

  A_list[[s]]   <- A_i
  Phi_list[[s]] <- Phi_i
  Psi_list[[s]]<- Psi_i
  X_list[[s]] <- simulate_subject(T, A_i, Phi_i, Psi)
}

```

random entries, not using normal, nonzero in order to be detected, maybe
uniform

# (4). Put into long format

```{r}
to_long <- function(X_list) {
  do.call(rbind, lapply(seq_along(X_list), function(id) {
    X <- X_list[[id]]
    data.frame(
      id   = id,
      time = 1:nrow(X),
      X
    )
  }))
}

dat_long <- to_long(X_list)

head(dat_long)
tail(dat_long)

```

#2. GIMME Simualation

```         
#########  GIMME Simulation. ############
#########################################
#########################################
```

### (1). Exporting simulated data to local (subject-level)

```{r}
length(X_list)
dim(X_list[[1]])
dir.create("sim_data", showWarnings = FALSE)

for (s in seq_along(X_list)) {
  fname <- file.path("sim_data", paste0("sub", s, ".txt"))
  write.table(
    X_list[[s]],
    file      = fname,
    row.names = FALSE,
    col.names = TRUE,
    sep       = " "   
  )
}
```

### (2). Run GIMME, export the results to local

```{r}
library(gimme)
dir.create("sim_results", showWarnings = FALSE)

fit <- gimme(          
  data      = "sim_data",  
  out       = "sim_results",
  sep       = "",           
  header    = TRUE,     
  ar        = TRUE,         
  plot      = FALSE,        
  subgroup  = FALSE,       
  paths     = NULL,         
  groupcutoff = .75,
  subcutoff   = .50,
  standardize = TRUE
)
```

### (3). True vs. Estimated group-level matrices

```{r}
library(dplyr)
library(tidyr)
library(readr)
library(qgraph)


###. Compare true vs estimated group-level matrices

list.files("sim_results")
PCM <- as.matrix(read.csv("sim_results/summaryPathCountsMatrix.csv",
                          check.names = FALSE))
head(PCM)

p <- nrow(PCM)
lag_cols <- grep("lag", colnames(PCM))
Phi_counts <- PCM[, lag_cols, drop = FALSE]
A_counts <- PCM[, -lag_cols, drop = FALSE]
N_subj <- max(PCM)
group_cutoff <- 0.75
Phi_est <- (Phi_counts >= group_cutoff * N_subj)
A_est   <- (A_counts  >= group_cutoff * N_subj)
Phi_est <- +Phi_est
A_est   <- +A_est

rownames(A_est)   <- rownames(PCM)
colnames(A_est)   <- gsub("lag", "", colnames(A_counts))
rownames(Phi_est) <- rownames(PCM)
colnames(Phi_est) <- gsub("lag", "", colnames(Phi_counts))
diag(A_est) <- 0
A_est
Phi_est



compare_mats <- function(M_true, M_est, tol = 1e-8) {
  
  # treat any nonzero entry as an edge
  true_edge <- abs(M_true) > tol
  est_edge  <- abs(M_est)  > tol
  
  TP <- sum(true_edge & est_edge)
  FP <- sum(!true_edge & est_edge)
  FN <- sum(true_edge & !est_edge)
  TN <- sum(!true_edge & !est_edge)
  
  data.frame(
    TP = TP,
    FP = FP,
    FN = FN,
    TN = TN,
    sensitivity = ifelse((TP + FN) == 0, NA, TP / (TP + FN)),
    specificity = ifelse((TN + FP) == 0, NA, TN / (TN + FP)),
    precision   = ifelse((TP + FP) == 0, NA, TP / (TP + FP))
  )
}


cat("===== Group-level Contemporaneous A =====\n")
compare_mats(A_common, A_est)

cat("===== Group-level Lagged Φ =====\n")
compare_mats(Phi_common, Phi_est)
```

### (4). Visualization

```{r}
library(qgraph)

layout_mat <- "spring"   # or "circle"

## A: contemporaneous
par(mfrow = c(1, 2))
qgraph(A_common,
       layout = layout_mat,
       edge.labels = TRUE,
       title = "True A (contemporaneous)")

qgraph(A_est,
       layout = layout_mat,
       edge.labels = TRUE,
       title = "Estimated A (contemporaneous)")

## Φ: lagged
par(mfrow = c(1, 2))
qgraph(Phi_common,
       layout = layout_mat,
       edge.labels = TRUE,
       title = "True Phi (lagged)")

qgraph(Phi_est,
       layout = layout_mat,
       edge.labels = TRUE,
       title = "Estimated Phi (lagged)")

par(mfrow = c(1, 1))

```

1/3 talking about model, 1/3 what we accessed

more systematic datasets (20 times, so 60 times in total) try n=100,
T=50, n=100, T=150 n=100, T=200 replicate estimation 20 times

TP,TN, FP, FN, Sensitivity, Specificity,... avg of those 20 runs
